{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd982769-6f51-4fa7-885b-97b83dc586a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66777333-95a8-4665-bc19-076a2bd2978d",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "\n",
    "In this exercise, you'll practice using BeautifulSoup to parse the content of a web page. The page that you'll be scraping, https://realpython.github.io/fake-jobs/, contains job listings. Your job is to extract the data on each job and convert into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb6a36-1fa7-45ec-9a20-6c2cc6f53407",
   "metadata": {},
   "source": [
    "1. Start by performing a GET request on the url above and convert the response into a BeautifulSoup object.  \n",
    "a. Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Hint: can you find a tag type and/or a class that could be helpful for extracting this information? Extract the text from this title.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb85aff5-43bf-4a22-abfb-7ef100c3d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in website for webscraping\n",
    "URL = 'https://realpython.github.io/fake-jobs/'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"MyPythonScraper\"\n",
    "}\n",
    "\n",
    "response = requests.get(URL, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c4b717-9dfc-4efe-9aaa-304b75278a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for connection\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3214e7-e3b9-484f-a908-c03c7c90f01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html>\\n  <head>\\n    <meta charset=\"utf-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n    <title>Fake Python</title>\\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bulma@0.9.2/css/bulma.min.css\">\\n  </head>\\n  <body>\\n  <section class=\"section\">\\n    <div class=\"container mb-5\">\\n      <h1 class=\"title is-1\">\\n        Fake Python\\n      </h1>\\n      <p class=\"subtitle is-3\">\\n        Fake Jobs for Your Web Scraping Journey\\n      </p>\\n    </div>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect source HTML\n",
    "response.text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6e4b0e-79e1-4132-96af-c877f88813fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'soup' object from fake jobs website scrape\n",
    "fake_jobs_soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f436b418-6299-479b-ba37-e8e629fd4600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Fake Python\n",
      "  </title>\n",
      "  <link href=\"https://cdn.jsdelivr.net/npm/bulma@0.9.2/css/bulma.min.css\" rel=\"stylesheet\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <section class=\"section\">\n",
      "   <div class=\"container mb-5\">\n",
      "    <h1 class=\"title is-1\">\n",
      "     Fake Python\n",
      "    </h1>\n",
      "    <p class=\"subtitle is-3\">\n",
      "     Fake Jobs for Your Web Scraping Journey\n",
      "    </p>\n",
      "   </div>\n",
      "   <div class=\"c\n"
     ]
    }
   ],
   "source": [
    "# display readable webscrape \n",
    "print(fake_jobs_soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067d623c-70a5-4221-90ae-a2ce708d3c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2 class=\"title is-5\">Senior Python Developer</h2>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# locate title for fake job we want\n",
    "fake_jobs_soup.find('h2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695c7d1-3f57-4731-9384-c791974e00cf",
   "metadata": {},
   "source": [
    "b. Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results in a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d72a3d0-6436-486c-bf8a-095a0d3dbfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Python Developer',\n",
       " 'Energy engineer',\n",
       " 'Legal executive',\n",
       " 'Fitness centre manager',\n",
       " 'Product manager',\n",
       " 'Medical technical officer',\n",
       " 'Physiological scientist',\n",
       " 'Textile designer',\n",
       " 'Television floor manager',\n",
       " 'Waste management officer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape and clean all fake job titles\n",
    "fake_job_titles = fake_jobs_soup.findAll('h2')\n",
    "job_names = [x.get_text(strip=True) for x in fake_job_titles]\n",
    "job_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c243f-11fd-43d5-ba89-05586c98c6f5",
   "metadata": {},
   "source": [
    "c. Finally, extract the companies, locations, and posting dates for each job. For example, the first job has a company of \"Payne, Roberts and Davis\", a location of \"Stewartbury, AA\", and a posting date of \"2021-04-08\". Ensure that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c867c468-6ccb-4a9d-83f5-8b74106cbaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>,\n",
       " <h3 class=\"subtitle is-6 company\">Vasquez-Davidson</h3>,\n",
       " <h3 class=\"subtitle is-6 company\">Jackson, Chambers and Levy</h3>,\n",
       " <h3 class=\"subtitle is-6 company\">Savage-Bradley</h3>,\n",
       " <h3 class=\"subtitle is-6 company\">Ramirez Inc</h3>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape all fake job companies\n",
    "fake_job_companies = fake_jobs_soup.findAll('h3')\n",
    "fake_job_companies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22dbce74-0cc4-4092-b23d-4ccd94f14e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Payne, Roberts and Davis',\n",
       " 'Vasquez-Davidson',\n",
       " 'Jackson, Chambers and Levy',\n",
       " 'Savage-Bradley',\n",
       " 'Ramirez Inc',\n",
       " 'Rogers-Yates',\n",
       " 'Kramer-Klein',\n",
       " 'Meyers-Johnson',\n",
       " 'Hughes-Williams',\n",
       " 'Jones, Williams and Villa']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regex and for loop to clean company names\n",
    "company_names =[]\n",
    "for company in fake_job_companies:\n",
    "    name = re.sub(r'<.*?>', '', str(company))\n",
    "    company_names.append(name)\n",
    "company_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120407e1-1f97-4904-a1ea-a8247b71bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stewartbury, AA',\n",
       " 'Christopherville, AA',\n",
       " 'Port Ericaburgh, AA',\n",
       " 'East Seanview, AP',\n",
       " 'North Jamieview, AP',\n",
       " 'Davidville, AP',\n",
       " 'South Christopher, AE',\n",
       " 'Port Jonathan, AE',\n",
       " 'Osbornetown, AE',\n",
       " 'Scotttown, AP']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape and clean all fake job locations\n",
    "fake_job_locations = fake_jobs_soup.findAll(attrs={'class' : 'location'})\n",
    "location_names = [x.get_text(strip=True) for x in fake_job_locations]\n",
    "location_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a60306dc-4b39-4d67-a7c7-ae0208db8cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape and clean all fake job datetimes\n",
    "fake_job_datetimes = fake_jobs_soup.findAll('time')\n",
    "dates = [x.get_text(strip=True) for x in fake_job_datetimes]\n",
    "dates[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c102cbf-0241-40e4-864f-d46b783205ee",
   "metadata": {},
   "source": [
    "d. Take the lists that you have created and combine them into a pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e391ff95-f984-44a1-b07b-653b66d7ac16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Job Title                Company Name              Location  \\\n",
       "0  Senior Python Developer    Payne, Roberts and Davis       Stewartbury, AA   \n",
       "1          Energy engineer            Vasquez-Davidson  Christopherville, AA   \n",
       "2          Legal executive  Jackson, Chambers and Levy   Port Ericaburgh, AA   \n",
       "3   Fitness centre manager              Savage-Bradley     East Seanview, AP   \n",
       "4          Product manager                 Ramirez Inc   North Jamieview, AP   \n",
       "\n",
       "         Date  \n",
       "0  2021-04-08  \n",
       "1  2021-04-08  \n",
       "2  2021-04-08  \n",
       "3  2021-04-08  \n",
       "4  2021-04-08  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Job Title': job_names, \n",
    "    'Company Name': company_names, \n",
    "    'Location': location_names, \n",
    "    'Date': dates\n",
    "}\n",
    "\n",
    "fake_jobs_df = pd.DataFrame(data)\n",
    "fake_jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5edcb8-e7b2-4fa6-a7d0-be525fe5dcd5",
   "metadata": {},
   "source": [
    "2. Next, add a column that contains the url for the \"Apply\" button. Try this in two ways.   \n",
    "    a. First, use the BeautifulSoup find_all method to extract the urls.  \n",
    "    b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\" job and the \"Scientist, research (maths)\" job.\n",
    "    \n",
    "3. Finally, we want to get the job description text for each job.  \n",
    "    a. Start by looking at the page for the first job, https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html. Using BeautifulSoup, extract the job description paragraph.  \n",
    "    b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the description text on that page. For example, if you input \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\" into your function, it should return the string \"At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.\".  \n",
    "    c. Use the [.apply method](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) on the url column you created above to retrieve the description text for all of the jobs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
